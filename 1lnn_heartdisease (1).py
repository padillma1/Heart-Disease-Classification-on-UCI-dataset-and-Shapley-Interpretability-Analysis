# -*- coding: utf-8 -*-
"""1LNN_HeartDisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ih2lYTizpCY3pLRNxekOpTaApn4blh3J
"""

pip install shap

import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
import shap

dataset1 = pd.read_csv("processed.cleveland.data", header=None)  # Assuming no header in the dataset
dataset2 = pd.read_csv("processed.hungarian.data", header=None)
dataset3 = pd.read_csv("processed.switzerland.data", header=None)
dataset4 = pd.read_csv("processed.va.data", header=None)

# Print the number of instances (examples) and features in each dataset (center)
num_instances1, num_features1 = dataset1.shape
num_instances2, num_features2 = dataset2.shape
num_instances3, num_features3 = dataset3.shape
num_instances4, num_features4 = dataset4.shape
print("Number of instances dataset 1:", num_instances1)
print("Number of features dataset 1:", num_features1-1)

print("Number of instances dataset 2:", num_instances2)
print("Number of features dataset 2:", num_features2-1)

print("Number of instances dataset 3:", num_instances3)
print("Number of features dataset 3:", num_features3-1)

print("Number of instances dataset 4:", num_instances4)
print("Number of features dataset 4:", num_features4-1)

# Replace '?' with NaN and then drop rows with NaN values (FLamby method)
dataset1 = dataset1.replace("?", np.NaN).drop([10, 11, 12], axis=1).dropna(axis=0)
dataset2 = dataset2.replace("?", np.NaN).drop([10, 11, 12], axis=1).dropna(axis=0)
dataset3 = dataset3.replace("?", np.NaN).drop([10, 11, 12], axis=1).dropna(axis=0)
dataset4 = dataset4.replace("?", np.NaN).drop([10, 11, 12], axis=1).dropna(axis=0)


dataset1 = dataset1.apply(pd.to_numeric)
dataset2 = dataset2.apply(pd.to_numeric)
dataset3 = dataset3.apply(pd.to_numeric)
dataset4 = dataset4.apply(pd.to_numeric)

num_instances1, num_features1 = dataset1.shape
num_instances2, num_features2 = dataset2.shape
num_instances3, num_features3 = dataset3.shape
num_instances4, num_features4 = dataset4.shape
print("Number of instances dataset 1:", num_instances1)
print("Number of features dataset 1:", num_features1-1)

print("Number of instances dataset 2:", num_instances2)
print("Number of features dataset 2:", num_features2-1)

print("Number of instances dataset 3:", num_instances3)
print("Number of features dataset 3:", num_features3-1)

print("Number of instances dataset 4:", num_instances4)
print("Number of features dataset 4:", num_features4-1)

# Concatenate the datasets vertically (row-wise)
combined_dataset = pd.concat([dataset1, dataset2, dataset3, dataset4], ignore_index=True)

# Add feature labels for the remaining 10 features and the label column
feature_labels = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak']
label = 'num'  # The label column

# Assign feature labels
combined_dataset.columns = feature_labels + [label]


# Print the number of instances (examples) and features in the combined data
num_instances, num_features = combined_dataset.shape
print("Number of instances:", num_instances)
print("Number of features:", num_features-1)
print(combined_dataset.columns)

# Add the column name 'label' to the top of the combined label column
#combined_dataset.columns = ['label'] + list(range(1, combined_dataset.shape[1]))

X = combined_dataset.iloc[:,:-1]
y = combined_dataset.iloc[:,-1].values
label_column = combined_dataset.iloc[:, -1]  # Extract the last column
# Print the range of values in the last column
print("Range of values in the last column:")
print(label_column.describe())
print(combined_dataset.columns)

# Convert labels > 0 to 1
label_column.where(y == 0, 1, inplace=True)
label_column = combined_dataset.iloc[:, -1]  # Extract the last column


# Print the range of values in the last column
print("Range of values in the last column:")
print(label_column.describe())

import torch
import torch.nn as nn
import torch.optim as optim

class Baseline(nn.Module):
   def __init__(self, input_dim=10, hidden_dim=16, output_dim=1):
        super(Baseline, self).__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_dim, output_dim)

   def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return torch.sigmoid(x)

import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

#np.random.seed(42)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.66, random_state=42)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Create DataLoader instances
batch_size = 4
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Initialize model, loss function, and optimizer
model = Baseline(input_dim=10, hidden_dim=16, output_dim=1)
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training the model with batch size of 4
#batch_size = 4
# Training loop
num_epochs = 30
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    all_labels = []
    all_preds = []

    for inputs, labels in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)

        # Collect predictions and labels for accuracy calculation
        all_labels.append(labels.detach().cpu())
        all_preds.append(outputs.detach().cpu())

    epoch_loss = running_loss / len(train_loader.dataset)

    # Compute accuracy
    all_labels = torch.cat(all_labels).numpy()
    all_preds = torch.cat(all_preds).numpy()
    all_preds = (all_preds > 0.5).astype(float)  # Convert probabilities to binary labels
    accuracy = accuracy_score(all_labels, all_preds)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}')

# Testing the model
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    y_pred_class = (y_pred > 0.5).float()

# Calculate accuracy on the test set
test_accuracy = accuracy_score(y_test_tensor.numpy(), y_pred_class.numpy())
print("Test Accuracy:", test_accuracy)

import shap

# SHAP values computation
def model_predict(X):
    # Convert DataFrame to NumPy array
    X_numpy = X.values if isinstance(X, pd.DataFrame) else X
    X_tensor = torch.tensor(X_numpy, dtype=torch.float32)
    with torch.no_grad():
        return model(X_tensor).numpy()

# Initialize the SHAP explainer with the model's prediction function
explainer = shap.Explainer(model_predict, X_train)
shap_values = explainer(X_train)

# Visualize SHAP values using a bar plot
shap.plots.bar(shap_values)

mlp_model = MLPClassifier(hidden_layer_sizes=(32,), max_iter=1000, random_state=42)

# Fit the model to the training data
mlp_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = mlp_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)